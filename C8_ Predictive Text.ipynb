{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 05_02 Preparing Data for Predictive Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text read from file :  In this practical, hands-on course, learn how to do data preparation, data munging, data visualization, and predictive analytics. \n",
      "PHP is the most popular server-side language used to build dynamic we\n",
      "\n",
      "Sample token list :  ['in', 'this', 'practical', 'hands-on', 'course', 'learn', 'how', 'to', 'do', 'data']\n",
      "\n",
      "Total Tokens :  579\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "#Read course description file\n",
    "base_file = open(\"Course-Descriptions.txt\", 'rt')\n",
    "raw_text = base_file.read()\n",
    "base_file.close()\n",
    "print(\"Text read from file : \",raw_text[:200])\n",
    "\n",
    "#tokenization\n",
    "import nltk\n",
    "token_list = nltk.word_tokenize(raw_text)\n",
    "\n",
    "#Replace special characters\n",
    "token_list2 = [word.replace(\"'\", \"\") for word in token_list ]\n",
    "\n",
    "#Remove punctuations\n",
    "token_list3 = list(filter(lambda token: nltk.tokenize.punkt.PunktToken(token).is_non_punct, token_list2))\n",
    "\n",
    "#Convert to lower case\n",
    "token_list4=[word.lower() for word in token_list3 ]\n",
    "\n",
    "print(\"\\nSample token list : \", token_list4[:10])\n",
    "print(\"\\nTotal Tokens : \",len(token_list4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 05_03 Building the ngrams DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FIRST= in SECOND= this COUNT= 2\n",
      "FIRST= this SECOND= practical COUNT= 1\n",
      "FIRST= practical SECOND= hands-on COUNT= 1\n",
      "FIRST= hands-on SECOND= course COUNT= 1\n",
      "FIRST= course SECOND= learn COUNT= 1\n"
     ]
    }
   ],
   "source": [
    "from nltk.util import ngrams\n",
    "\n",
    "#Use a sqlite database to store ngrams information\n",
    "import sqlite3\n",
    "conn = sqlite3.connect(\":memory:\")\n",
    "\n",
    "#table to store first word, second word and count of occurance\n",
    "conn.execute('''DROP TABLE IF EXISTS NGRAMS''')\n",
    "conn.execute('''CREATE TABLE NGRAMS \n",
    "         (FIRST   TEXT  NOT NULL,\n",
    "          SECOND  TEXT  NOT NULL,\n",
    "          COUNTS  INT   NOT NULL,\n",
    "         CONSTRAINT PK_GRAMS PRIMARY KEY (FIRST,SECOND));''')\n",
    "\n",
    "#Generate bigrams\n",
    "bigrams = ngrams(token_list4,2)\n",
    "\n",
    "#Store bigrams in DB\n",
    "for i in bigrams:\n",
    "    insert_str=\"INSERT INTO NGRAMS (FIRST,SECOND,COUNTS) \\\n",
    "          VALUES ('\" + i[0] + \"','\" + i[1] + \"',1 ) \\\n",
    "          ON CONFLICT(FIRST,SECOND) DO UPDATE SET COUNTS=COUNTS + 1\"   \n",
    "    conn.execute(insert_str);\n",
    "\n",
    "#Look at sample data from the table\n",
    "cursor = conn.execute(\"SELECT FIRST, SECOND, COUNTS from NGRAMS LIMIT 5\")\n",
    "for gram_row in cursor:\n",
    "    print(\"FIRST=\", gram_row[0], \"SECOND=\",gram_row[1],\"COUNT=\",gram_row[2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 05_04 Recommending next word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next word for data  ['science', 'analysis', 'data', 'from', 'in', 'mining', 'munging', 'node.js', 'preparation', 'scientists', 'visualization', 'you']\n",
      "\n",
      "Next word for science  ['begins', 'requires', 'specialists', 'teams']\n"
     ]
    }
   ],
   "source": [
    "#Function to query DB and find next word\n",
    "def recommend(str):\n",
    "    nextwords = []\n",
    "    #Find next words, sort them by most occurance\n",
    "    cur_filter = conn.execute(\"SELECT SECOND from NGRAMS \\\n",
    "                              WHERE FIRST='\" + str + \"' \\\n",
    "                              ORDER BY COUNTS DESC\")\n",
    "    \n",
    "    #Build a list ordered from most frequent to least frequent next word\n",
    "    for filt_row in cur_filter:\n",
    "        nextwords.append(filt_row[0])\n",
    "    return nextwords\n",
    "\n",
    "#Recommend for words data and science\n",
    "print(\"Next word for data \", recommend(\"data\"))\n",
    "print(\"\\nNext word for science \", recommend(\"science\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
