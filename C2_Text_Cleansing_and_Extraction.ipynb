{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 03_01 Tokenization\n",
    "\n",
    "Tokenization refers to converting a text string into individual tokens. Tokens may be words or punctations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token List :  ['Data', 'science', 'is', 'the', 'study', 'of', 'data', 'to', 'extract', 'meaningful', 'insights', 'for', 'business', '.', 'It', 'is', 'a', 'multidisciplinary', 'approach', 'that']\n",
      "\n",
      " Total Tokens :  158\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the base file into a raw text variable\n",
    "base_file = open(os.getcwd()+ \"/data_science.txt\", 'rt')\n",
    "raw_text = base_file.read()\n",
    "base_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token List :  ['Data', 'science', 'is', 'the', 'study', 'of', 'data', 'to', 'extract', 'meaningful', 'insights', 'for', 'business', '.', 'It', 'is', 'a', 'multidisciplinary', 'approach', 'that']\n",
      "\n",
      " Total Tokens :  158\n"
     ]
    }
   ],
   "source": [
    "#Extract tokens\n",
    "token_list = nltk.word_tokenize(raw_text)\n",
    "print(\"Token List : \",token_list[:20])\n",
    "print(\"\\n Total Tokens : \",len(token_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 03_02 Cleansing Text\n",
    "\n",
    "We will see examples of removing punctuation and converting to lower case\n",
    "\n",
    "#### Remove Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token List after removing punctuation :  ['Data', 'science', 'is', 'the', 'study', 'of', 'data', 'to', 'extract', 'meaningful', 'insights', 'for', 'business', 'It', 'is', 'a', 'multidisciplinary', 'approach', 'that', 'combines']\n",
      "\n",
      "Total tokens after removing punctuation :  136\n"
     ]
    }
   ],
   "source": [
    "#Use the Punkt library to extract tokens\n",
    "token_list2 = list(filter(lambda token: nltk.tokenize.punkt.PunktToken(token).is_non_punct, token_list))\n",
    "print(\"Token List after removing punctuation : \",token_list2[:20])\n",
    "print(\"\\nTotal tokens after removing punctuation : \", len(token_list2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert to Lower Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token list after converting to lower case :  ['data', 'science', 'is', 'the', 'study', 'of', 'data', 'to', 'extract', 'meaningful', 'insights', 'for', 'business', 'it', 'is', 'a', 'multidisciplinary', 'approach', 'that', 'combines']\n",
      "\n",
      "Total tokens after converting to lower case :  136\n"
     ]
    }
   ],
   "source": [
    "token_list3=[word.lower() for word in token_list2 ]\n",
    "print(\"Token list after converting to lower case : \", token_list3[:20])\n",
    "print(\"\\nTotal tokens after converting to lower case : \", len(token_list3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 03_03 Stop word Removal\n",
    "\n",
    "Removing stop words by using a standard stop word list available in NLTK for English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token list after removing stop words :  ['data', 'science', 'study', 'data', 'extract', 'meaningful', 'insights', 'business', 'multidisciplinary', 'approach', 'combines', 'principles', 'practices', 'fields', 'mathematics', 'statistics', 'artificial', 'intelligence', 'computer', 'engineering']\n",
      "\n",
      "Total tokens after removing stop words :  79\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "#Download the standard stopword list\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#Remove stopwords\n",
    "token_list4 = list(filter(lambda token: token not in stopwords.words('english'), token_list3))\n",
    "print(\"Token list after removing stop words : \", token_list4[:20])\n",
    "print(\"\\nTotal tokens after removing stop words : \", len(token_list4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 03_04 Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token list after stemming :  ['data', 'scienc', 'studi', 'data', 'extract', 'meaning', 'insight', 'busi', 'multidisciplinari', 'approach', 'combin', 'principl', 'practic', 'field', 'mathemat', 'statist', 'artifici', 'intellig', 'comput', 'engin']\n",
      "\n",
      "Total tokens after Stemming :  79\n"
     ]
    }
   ],
   "source": [
    "#Use the PorterStemmer library for stemming.\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "#Stem data\n",
    "token_list5 = [stemmer.stem(word) for word in token_list4 ]\n",
    "print(\"Token list after stemming : \", token_list5[:20])\n",
    "print(\"\\nTotal tokens after Stemming : \", len(token_list5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 03_05 Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Use the wordnet library to map words to their lemmatized form\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token list after Lemmatization :  ['data', 'science', 'study', 'data', 'extract', 'meaningful', 'insight', 'business', 'multidisciplinary', 'approach', 'combine', 'principle', 'practice', 'field', 'mathematics', 'statistic', 'artificial', 'intelligence', 'computer', 'engineering']\n",
      "\n",
      "Total tokens after Lemmatization :  79\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "token_list6 = [lemmatizer.lemmatize(word) for word in token_list4 ]\n",
    "print(\"Token list after Lemmatization : \", token_list6[:20])\n",
    "print(\"\\nTotal tokens after Lemmatization : \", len(token_list6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparison of tokens between raw, stemming and lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw :  data  , Stemmed :  data  , Lemmatized :  data\n",
      "Raw :  science  , Stemmed :  scienc  , Lemmatized :  science\n",
      "Raw :  study  , Stemmed :  studi  , Lemmatized :  study\n",
      "Raw :  data  , Stemmed :  data  , Lemmatized :  data\n",
      "Raw :  extract  , Stemmed :  extract  , Lemmatized :  extract\n",
      "Raw :  meaningful  , Stemmed :  meaning  , Lemmatized :  meaningful\n",
      "Raw :  insights  , Stemmed :  insight  , Lemmatized :  insight\n",
      "Raw :  business  , Stemmed :  busi  , Lemmatized :  business\n",
      "Raw :  multidisciplinary  , Stemmed :  multidisciplinari  , Lemmatized :  multidisciplinary\n",
      "Raw :  approach  , Stemmed :  approach  , Lemmatized :  approach\n",
      "Raw :  combines  , Stemmed :  combin  , Lemmatized :  combine\n",
      "Raw :  principles  , Stemmed :  principl  , Lemmatized :  principle\n",
      "Raw :  practices  , Stemmed :  practic  , Lemmatized :  practice\n",
      "Raw :  fields  , Stemmed :  field  , Lemmatized :  field\n",
      "Raw :  mathematics  , Stemmed :  mathemat  , Lemmatized :  mathematics\n",
      "Raw :  statistics  , Stemmed :  statist  , Lemmatized :  statistic\n",
      "Raw :  artificial  , Stemmed :  artifici  , Lemmatized :  artificial\n",
      "Raw :  intelligence  , Stemmed :  intellig  , Lemmatized :  intelligence\n",
      "Raw :  computer  , Stemmed :  comput  , Lemmatized :  computer\n",
      "Raw :  engineering  , Stemmed :  engin  , Lemmatized :  engineering\n"
     ]
    }
   ],
   "source": [
    "#Check for token technlogies\n",
    "for i in range(20):\n",
    "    print( \"Raw : \", token_list4[i],\" , Stemmed : \", token_list5[i], \" , Lemmatized : \", token_list6[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
